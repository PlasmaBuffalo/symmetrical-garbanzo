{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "The goal of this milestone is for you to scrape data from an actual website and start to analyze the results. The project should be completed in individually in python without the assistance of any artificial intelligences.\n",
    "\n",
    "Web Scraper - Take all of the text from the top 20 articles coming from one of the following web news sources:\n",
    "\n",
    "espn.com\n",
    "cnn.com\n",
    "goodblacknews.org\n",
    "huffingtonpost.com\n",
    "ign.com\n",
    "theonion.com\n",
    "\n",
    "Statistics - Print a list of the titles of the web pages you are pulling text from and then print the mean and median number of words for all 20 articles.\n",
    "\n",
    "Visualize - Create a Word Cloud using your list of most common words that shows the top 50 (or up to 200) words and a bar chart to show the relative frequencies of the top 15 most frequent words. (Note: these should be words that the average viewer of the website would see, not code from the html)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A bit of code: downloads the linked HTML page into this notebook's folder\n",
    "#code example is from https://pythonexamples.org/python-download-from-url/\n",
    "\"\"\"import requests\n",
    "\n",
    "URL = \"https://pythonexamples.org/\"\n",
    "response = requests.get(URL)\n",
    "\n",
    "with open(\"download.html\", \"wb\") as htmlFile:\n",
    "    htmlFile.write(response.content)\n",
    "    print('Download completed.')\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan:\n",
    "-Download main webpage: https://www.cnn.com\n",
    "-Have it figure out what is an article and what is not\n",
    "-Generate a list of links to articles\n",
    "-For the first 20 relevant links, download each link's HTML to this folder\n",
    "-For each page, I need: page title, word count, list of all words\n",
    "-This comes from <title>,<h1-h6>,<li>,<p>,<a>,<article>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code that downloads the main CNN page as a file\n",
    "#help coming from https://lxml.de/parsing.html#parsers for lxml documentation\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "base_url = \"https://www.cnn.com\"\n",
    "response = requests.get(base_url)\n",
    "#response encoding is utf-8\n",
    "\n",
    "with open(\"main_page.html\", \"wb\") as htmlFile:\n",
    "    htmlFile.write(response.content)\n",
    "\n",
    "#now we have main page downloaded, and we can parse it for article links\n",
    "#THE ENCODING PART HERE IS REALLY IMPORTANT OR ELSE NOTHING WORKS\n",
    "fileWrap = open(\"main_page.html\",\"r\", encoding=\"utf-8\")\n",
    "\n",
    "#this converts the open file into a single string variable\n",
    "fileText:str = fileWrap.read()\n",
    "\n",
    "#turning the downloaded file into a BS4 object\n",
    "bsText = bs(fileText,\"html.parser\")\n",
    "\n",
    "#list to store all valid article links\n",
    "articles = []\n",
    "\n",
    "for link in bsText.find_all('a'):\n",
    "    currentLink:str = link.get('href')\n",
    "    try:\n",
    "        #pattern to save: all articles start with \"/\" and end with \".html\"\n",
    "        if(currentLink.startswith('/') and currentLink.endswith('.html')):\n",
    "            articles.append(base_url+currentLink)\n",
    "    except:\n",
    "        print(\"not that one\")\n",
    "        articles.remove\n",
    "\n",
    "\"\"\" \n",
    "for link in articles:\n",
    "    print(link)\n",
    " \"\"\"\n",
    "# now that we have all the articles, we pick 20 at random and analyze the results\n",
    "\n",
    "wordsList = {}\n",
    "for i in range(len(articles)):\n",
    "#turns the entire request page into a BS4 object\n",
    "    thisArticle = bs(requests.get(articles[i]).text, \"html.parser\")\n",
    "\n",
    "    articleText = thisArticle.article.text.strip().casefold()\n",
    "    #now narrow down the text we're interested in to just the article itself\n",
    "    #NLTK functions found on google and configured through documentation from https://www.nltk.org/\n",
    "    thisArticle = ' '.join([word for word in articleText.split()])\n",
    "    wordTokens = nltk.word_tokenize(thisArticle)\n",
    "\n",
    "    for word in wordTokens:\n",
    "        # if the current word already exists as a key in the wordsList\n",
    "        # if(word in wordsList.keys()):\n",
    "        if word in wordsList:\n",
    "            wordsList[word] += 1\n",
    "        else:\n",
    "            wordsList[word] = 1\n",
    "\n",
    "# Sort the word counts in descending order.\n",
    "# assistance received from https://realpython.com/sort-python-dictionary/ for figuring out parameters\n",
    "wordsList = sorted(wordsList.items(), key=lambda item: item[1], reverse=True),\n",
    "\n",
    "print(wordsList)\n",
    "for item in wordsList:\n",
    "    print(item)\n",
    "\n",
    "#too many words to fit on the frequency chart, now we cut the end of the list\n",
    "print(\"Length=\",len(wordsList))\n",
    "while (len(wordsList)>50):\n",
    "    wordsList.pop()\n",
    "print(\"Length=\",len(wordsList.count()))\n",
    "\n",
    "# Create a bar chart.\n",
    "plt.bar([word for word, count in wordsList], [count for word, count in wordsList])\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Word Frequency Chart\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
