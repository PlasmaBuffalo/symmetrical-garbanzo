{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "The goal of this milestone is for you to scrape data from an actual website and start to analyze the results. The project should be completed in individually in python without the assistance of any artificial intelligences.\n",
    "\n",
    "Web Scraper - Take all of the text from the top 20 articles coming from one of the following web news sources:\n",
    "\n",
    "espn.com\n",
    "cnn.com\n",
    "goodblacknews.org\n",
    "huffingtonpost.com\n",
    "ign.com\n",
    "theonion.com\n",
    "\n",
    "Statistics - Print a list of the titles of the web pages you are pulling text from and then print the mean and median number of words for all 20 articles.\n",
    "\n",
    "Visualize - Create a Word Cloud using your list of most common words that shows the top 50 (or up to 200) words and a bar chart to show the relative frequencies of the top 15 most frequent words. (Note: these should be words that the average viewer of the website would see, not code from the html)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import requests\\n\\nURL = \"https://pythonexamples.org/\"\\nresponse = requests.get(URL)\\n\\nwith open(\"download.html\", \"wb\") as htmlFile:\\n    htmlFile.write(response.content)\\n    print(\\'Download completed.\\')\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A bit of code: downloads the linked HTML page into this notebook's folder\n",
    "#code example is from https://pythonexamples.org/python-download-from-url/\n",
    "\"\"\"import requests\n",
    "\n",
    "URL = \"https://pythonexamples.org/\"\n",
    "response = requests.get(URL)\n",
    "\n",
    "with open(\"download.html\", \"wb\") as htmlFile:\n",
    "    htmlFile.write(response.content)\n",
    "    print('Download completed.')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan:\n",
    "-Download main webpage: https://www.cnn.com\n",
    "-Have it figure out what is an article and what is not\n",
    "-Generate a list of links to articles\n",
    "-For the first 20 relevant links, download each link's HTML to this folder\n",
    "-For each page, I need: page title, word count, list of all words\n",
    "-This comes from <title>,<h1-h6>,<li>,<p>,<a>,<article>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 100466: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\liamz\\Documents\\GitHub\\symmetrical-garbanzo\\CS360\\Project 1 Milestone 2\\p1m2.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/liamz/Documents/GitHub/symmetrical-garbanzo/CS360/Project%201%20Milestone%202/p1m2.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#now we have main page downloaded, and we can parse it for article links\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/liamz/Documents/GitHub/symmetrical-garbanzo/CS360/Project%201%20Milestone%202/p1m2.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m fileAsIOWrapper \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmain_page.html\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/liamz/Documents/GitHub/symmetrical-garbanzo/CS360/Project%201%20Milestone%202/p1m2.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m fileAsText \u001b[39m=\u001b[39m fileAsIOWrapper\u001b[39m.\u001b[39;49mread()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/liamz/Documents/GitHub/symmetrical-garbanzo/CS360/Project%201%20Milestone%202/p1m2.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m tree \u001b[39m=\u001b[39m etree\u001b[39m.\u001b[39mfromstring(\u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmain_page.html\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mread(),\u001b[39m\"\u001b[39m\u001b[39mparser=html.parser\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\liamz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39mcharmap_decode(\u001b[39minput\u001b[39m,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors,decoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 100466: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "#Code that downloads the main CNN page as a file\n",
    "#help coming from https://lxml.de/parsing.html#parsers for lxml documentation\n",
    "import requests\n",
    "from lxml import etree\n",
    "from io import StringIO, BytesIO\n",
    "\n",
    "base_url = \"https://www.cnn.com\"\n",
    "response = requests.get(base_url)\n",
    "\n",
    "with open(\"main_page.html\", \"wb\") as htmlFile:\n",
    "    htmlFile.write(response.content)\n",
    "\n",
    "#now we have main page downloaded, and we can parse it for article links\n",
    "fileAsIOWrapper = open(\"main_page.html\",\"r\")\n",
    "fileAsText = fileAsIOWrapper.read()\n",
    "\n",
    "tree = etree.fromstring(open(\"main_page.html\",\"r\").read(),\"parser=html.parser\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
