{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "The goal of this milestone is for you to scrape data from an actual website and start to analyze the results. The project should be completed in individually in python without the assistance of any artificial intelligences.\n",
    "\n",
    "Web Scraper - Take all of the text from the top 20 articles coming from one of the following web news sources:\n",
    "\n",
    "espn.com\n",
    "cnn.com\n",
    "goodblacknews.org\n",
    "huffingtonpost.com\n",
    "ign.com\n",
    "theonion.com\n",
    "\n",
    "Statistics - Print a list of the titles of the web pages you are pulling text from and then print the mean and median number of words for all 20 articles.\n",
    "\n",
    "Visualize - Create a Word Cloud using your list of most common words that shows the top 50 (or up to 200) words and a bar chart to show the relative frequencies of the top 15 most frequent words. (Note: these should be words that the average viewer of the website would see, not code from the html)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A bit of code: downloads the linked HTML page into this notebook's folder\n",
    "#code example is from https://pythonexamples.org/python-download-from-url/\n",
    "\"\"\"import requests\n",
    "\n",
    "URL = \"https://pythonexamples.org/\"\n",
    "response = requests.get(URL)\n",
    "\n",
    "with open(\"download.html\", \"wb\") as htmlFile:\n",
    "    htmlFile.write(response.content)\n",
    "    print('Download completed.')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan:\n",
    "-Download main webpage: https://www.cnn.com\n",
    "-Have it figure out what is an article and what is not\n",
    "-Generate a list of links to articles\n",
    "-For the first 20 relevant links, download each link's HTML to this folder\n",
    "-For each page, I need: page title, word count, list of all words\n",
    "-This comes from <title>,<h1-h6>,<li>,<p>,<a>,<article>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\liamz\\AppData\\Local\\Programs\\Python\\Python310\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/liamz/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#Code that downloads the main CNN page as a file\n",
    "#help coming from https://lxml.de/parsing.html#parsers for lxml documentation\n",
    "import requests\n",
    "from lxml import etree\n",
    "from io import StringIO, BytesIO\n",
    "\n",
    "base_url = \"https://www.cnn.com\"\n",
    "response = requests.get(base_url)\n",
    "\n",
    "with open(\"main_page.html\", \"wb\") as htmlFile:\n",
    "    htmlFile.write(response.content)\n",
    "\n",
    "#now we have main page downloaded, and we can parse it for article links\n",
    "fileAsIOWrapper = open(\"main_page.html\",\"r\")\n",
    "fileAsText = fileAsIOWrapper.read()\n",
    "\n",
    "tree = etree.fromstring(open(\"main_page.html\",\"r\").read(),\"parser=html.parser\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
