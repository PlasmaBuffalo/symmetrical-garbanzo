{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this milestone is for you to scrape data from an actual website and start to analyze the results. The project should be completed in individually in python without the assistance of any artificial intelligences.\n",
    "\n",
    "Web Scraper - Take all of the text from the top 20 articles coming from one of the following web news sources:\n",
    "\n",
    "espn.com\n",
    "cnn.com\n",
    "goodblacknews.org\n",
    "huffingtonpost.com\n",
    "ign.com\n",
    "theonion.com\n",
    "\n",
    "Statistics - Print a list of the titles of the web pages you are pulling text from and then print the mean and median number of words for all 20 articles.\n",
    "\n",
    "Visualize - Create a Word Cloud using your list of most common words that shows the top 50 (or up to 200) words and a bar chart to show the relative frequencies of the top 15 most frequent words. (Note: these should be words that the average viewer of the website would see, not code from the html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDS Project 1 Milestone 3\n",
    "# Liam Zalubas\n",
    "# help coming from https://lxml.de/parsing.html#parsers for lxml documentation\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as sia\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# change this to check more articles\n",
    "articlesToUse:int = 0\n",
    "wordsForCloud:int = 50\n",
    "wordsForGraph:int = 15\n",
    "base_url = \"https://www.cnn.com\"\n",
    "\n",
    "# some resources to use for string filtering/cleaning\n",
    "punc = str.maketrans('', '', string.punctuation)\n",
    "stop_words = stopwords.words(\"english\")\n",
    "# beautiful soup symbols to remove\n",
    "unwantedThings = {'“', '”', '–', '’s', '—', '’','•'}\n",
    "# list of words to add to stoplist, including CNN-specific page elements\n",
    "unwantedWords = {\"said\", \"feedback\", \"cnn\", \"source\", \"ad\", \"image\", \"video\"}\n",
    "# specific words to add to stopwords filter\n",
    "for word in unwantedWords:\n",
    "    stop_words.append(word)\n",
    "\n",
    "# addition of sentiment analysis tool\n",
    "sentiment_tool = sia()\n",
    "\n",
    "# start program here: get the main page from #! base_url\n",
    "response = requests.get(base_url)\n",
    "# response encoding is utf-8\n",
    "\n",
    "with open(\"main_page.html\", \"wb\") as htmlFile:\n",
    "    htmlFile.write(response.content)\n",
    "\n",
    "# now we have main page downloaded, and we can parse it for article links\n",
    "# THE ENCODING PART HERE IS REALLY IMPORTANT OR ELSE NOTHING WORKS\n",
    "fileWrap = open(\"main_page.html\", \"r\", encoding=\"utf-8\")\n",
    "\n",
    "# this converts the open file into a single string variable\n",
    "fileText: str = fileWrap.read()\n",
    "\n",
    "# turning the downloaded file into a BS4 object\n",
    "bsText = bs(fileText, \"html.parser\")\n",
    "\n",
    "# list to store all valid article links\n",
    "articles = []\n",
    "\n",
    "# get all the potential links that match patterns of articles\n",
    "for link in bsText.find_all('a'):\n",
    "    currentLink: str = link.get('href')\n",
    "    try:\n",
    "        # pattern to save: all articles start with \"/\" and end with \".html\"\n",
    "        if (currentLink.startswith('/') and currentLink.endswith('.html')):\n",
    "            articles.append(base_url+currentLink)\n",
    "    except:\n",
    "        print(\"#\")\n",
    "\n",
    "while (articlesToUse < 1 or articlesToUse > len(articles)):\n",
    "    articlesToUse = int(input(\"How many articles do you want to process?\\n\"))\n",
    "\n",
    "# now that we have all the articles, we pick {randomArticleCount:int} at random and analyze the results\n",
    "# start with a list to put our words and their frequencies in\n",
    "wordsList = {}\n",
    "\n",
    "# and one for the titles to use later\n",
    "titleArray = []\n",
    "\n",
    "#article word counts to use as data for statistics\n",
    "articleWordCounts = [0 for i in range(articlesToUse)]\n",
    "articleSentiment = []\n",
    "\n",
    "\n",
    "# method to scrub words and pass them to the adder method\n",
    "def cleanWord(thisWord:str, articleNum:int):\n",
    "    cleanedWord = thisWord\n",
    "\n",
    "    # remove all numbers: if a number is found, return without adding word\n",
    "    for num in range(0,9):\n",
    "        if (cleanedWord.find(str(num)) != -1):\n",
    "            return\n",
    "\n",
    "    # make it all lowercase and remove most punctuation\n",
    "    cleanedWord = cleanedWord.casefold().translate(punc)\n",
    "    # temporary fix for manually-identified punctuation outliers\n",
    "    for symbol in unwantedThings:\n",
    "        cleanedWord = cleanedWord.replace(symbol, ' ').strip()\n",
    "    # completely ditch the word if it's just an empty string\n",
    "    if (cleanedWord != ''):\n",
    "        # if the above operations turned the word into multiple words, recurse this method for each new word\n",
    "        if (cleanedWord.find(' ') != -1):\n",
    "            cleanedWord.split()\n",
    "            for i in range(len(cleanedWord)):\n",
    "                cleanWord(cleanedWord[i], articleNum)\n",
    "\n",
    "        # make sure the word is not a stopword before taking it into account\n",
    "        if cleanedWord not in stop_words:\n",
    "            incrementWord(cleanedWord, articleNum)\n",
    "    else:\n",
    "        return\n",
    "\n",
    "# this is a method to send valid words into wordsList or increment their count\n",
    "def incrementWord(thisWord: str, articleNum:int):\n",
    "    # add 1 to the article's word count\n",
    "    articleWordCounts[articleNum] += 1\n",
    "    # for each word key in the dictionary:\n",
    "    # if already exists, add one to the frequency counter\n",
    "    # else, add it to the words list and start at frequency=1\n",
    "    if thisWord in wordsList:\n",
    "        wordsList[thisWord] += 1\n",
    "    else:\n",
    "        wordsList[thisWord] = 1\n",
    "\n",
    "#randomize the links within the list, then use the first {articlesToUse:int} articles.\n",
    "np.random.shuffle(articles)\n",
    "\n",
    "for i in range(articlesToUse):\n",
    "    # turns the entire request page into a BS4 object\n",
    "    thisArticle = bs(requests.get(articles[i]).text, \"html.parser\")\n",
    "\n",
    "    # print the name of the article we're about to check\n",
    "    print(thisArticle.title.text, end=\" \")# type:ignore\n",
    "\n",
    "    titleArray.append((thisArticle.title.text).replace(\" | CNN\",\"\"))# type:ignore\n",
    "\n",
    "    # now narrow down the text we're interested in to just the article itself\n",
    "    # for each piece of text in a new tag from the last, insert a space between them\n",
    "    textBlocks = thisArticle.article.get_text(strip=True, separator='\\n').splitlines() # type:ignore\n",
    "\n",
    "    # make it all a single string (separated with spaces)\n",
    "    bigString:str = ''\n",
    "    for block in textBlocks:\n",
    "        bigString += ' ' + block\n",
    "\n",
    "    # here is where we run sentiment analysis, since we have proper sentence structure to pass to VADER\n",
    "    score = sentiment_tool.polarity_scores(bigString)\n",
    "    articleSentiment.append(score.get('compound'))\n",
    "    \"\"\"\n",
    "    From VADER site:\n",
    "    positive sentiment: compound score >= 0.05\n",
    "    neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "    negative sentiment: compound score <= -0.05\n",
    "    \"\"\"\n",
    "    if(articleSentiment[i] >= 0.05):\n",
    "        print(\"- Good Sentiment at compound score =\",articleSentiment[i])\n",
    "    elif(articleSentiment[i] <= -0.05):\n",
    "        print(\"- Bad Sentiment at compound score =\",articleSentiment[i])\n",
    "    else:\n",
    "        print(\"- Neutral Sentiment at compound score =\",articleSentiment[i])\n",
    "    # now we tokenize based on single words instead of blocks\n",
    "    bigTokens = nltk.word_tokenize(bigString)\n",
    "\n",
    "    # then iterate through all tokens we have\n",
    "    for word in bigTokens:\n",
    "        cleanWord(word, i)\n",
    "\n",
    "\n",
    "# Sort the word counts in descending order.\n",
    "sortedWordsList = sorted(wordsList.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "#print the mean and median values for word count\n",
    "print(\"\\nMean word count:\",np.average(articleWordCounts))\n",
    "for i in range(len(articleWordCounts)):\n",
    "    print(articleWordCounts[i],\"words in article:\",titleArray[i])\n",
    "print(\"Median word count:\",articleWordCounts[int(len(articleWordCounts)/2)])\n",
    "\n",
    "avg_sentiment = 0\n",
    "for sentiment in articleSentiment:\n",
    "    avg_sentiment += sentiment\n",
    "avg_sentiment /= articlesToUse\n",
    "\n",
    "print(\"Average sentiment:\",avg_sentiment)\n",
    "\n",
    "if avg_sentiment > 0:\n",
    "    print(\"Overall good sentiment!\")\n",
    "else:\n",
    "    print(\"Overall bad sentiment!\")\n",
    "\n",
    "# too many words to fit on the word cloud, here we cut the end of the list\n",
    "while (len(sortedWordsList) > wordsForCloud):\n",
    "    sortedWordsList.pop()\n",
    "\n",
    "# now we make the word cloud with 50 words\n",
    "# Create a WordCloud object.\n",
    "wordcloud = WordCloud()\n",
    "\n",
    "# Generate the word cloud.\n",
    "wordcloud.generate_from_frequencies(wordsList)\n",
    "\n",
    "# Display the word cloud.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Cloud\")\n",
    "plt.show()\n",
    "\n",
    "# now we drop more words to fit the bar chart\n",
    "while (len(sortedWordsList) > wordsForGraph):\n",
    "    sortedWordsList.pop()\n",
    "\n",
    "for word in sortedWordsList:\n",
    "    print(word)\n",
    "\n",
    "# Create a bar chart.\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.bar([word for word, count in sortedWordsList],\n",
    "        [count for word, count in sortedWordsList])\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Word Frequency Chart\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Written Analysis\n",
    "\n",
    "## Scenario 1: Imagine that you work for a company that sells a variety of tennis equipment and you could analyze the user reviews for each product in the same way you analyzed the articles from the above news sources. What could be the benefit of your analysis to your company? Is there a benefit your customers? If another site collected similar data, ratings of your products, who owns that data and what parties are involved?\n",
    "\n",
    "User reviews can be a valuable source of information for both companies and customers. For companies, user reviews can help identify product strengths and weaknesses (mainly through the extremes seen in sentiment analysis), track customer satisfaction over time (trends of sentiment for a product or series of products), identify common customer problems (common words paired with strong, negative sentiment), and benchmark products against competitors (do the same analysis for similar products). For customers, it depends on if they get to access the review analysis findings theirselves, or only get to benefit from what the company finds from this analysis. User reviews generally help customers make informed purchase decisions, identify products that are a good fit for their needs, and also get help with product troubleshooting if others have had similar issues. The data ownership and parties involved section concerns who owns the user reviews that are collected on websites and how that data is used. Primarily, the website where the reviews are collected owns the data, but the company that owns the products being reviewed may also use the data to improve its products and customer service. Additionally, third-party data analytics companies may analyze the reviews to provide insights to the website and the company that owns the products being reviewed. Customers who write reviews own their own reviews, but they may grant the website and the company that owns the products being reviewed a license to use the reviews depending on the terms of use for that specific site.\n",
    "\n",
    "## Scenario 2: Suppose you had access to the social media accounts of the people in your social circle and could analyze their posts in the same way you analyzed the articles from the above news sources. What do you think you could learn about each individual after doing this? How do you think they would feel about you doing that? and would it matter if you told them what you were doing or if they found out some other way? Would it make a difference if you could also analyze all of their private messages as well?\n",
    "\n",
    "While there's a lot of analysis that you can perform on public social media posts, I would find a few things to be most interesting: timing, location, and common word use data. Timing would involve the specific times people choose to post and/or comment using their account, letting you know when they most frequently use said social media account. Location data, assuming the media site contains it (such as snapchat or other services which tag your location at the time of a post) would narrow down exactly where someone is at a given time they use their account. Common word use could provide some unique insights into what a person is interested in, and would be very tempting to have if you were a company trying to sell a particular product to people. On one hand, I can understand people feeling exposed or uncomfortable with these kinds of data analysis, as it may feel like i'd be going too far into personal information. On the other hand, if this information was publicly available (as opposed to being from accounts which are invite-only/friends-only limited), I would be surprised if people called it an invasion of privacy. Telling someone about analyzing their data upfront would definitely feel less shady as opposed to having them find out some other way. This moderately ties into the idea of asking for forgiveness instead of permission: you wouldn't want to ruin your reputation with friends just because of a data analysis project in which you use their information without their knowledge rather than being upfront and honest. It would absolutely make a difference having private messages, and the amount of extra information that would bring in would be a whole new project in itself. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
