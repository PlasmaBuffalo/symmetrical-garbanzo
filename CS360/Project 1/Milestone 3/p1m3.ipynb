{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this milestone is for you to scrape data from an actual website and start to analyze the results. The project should be completed in individually in python without the assistance of any artificial intelligences.\n",
    "\n",
    "Web Scraper - Take all of the text from the top 20 articles coming from one of the following web news sources:\n",
    "\n",
    "espn.com\n",
    "cnn.com\n",
    "goodblacknews.org\n",
    "huffingtonpost.com\n",
    "ign.com\n",
    "theonion.com\n",
    "\n",
    "Statistics - Print a list of the titles of the web pages you are pulling text from and then print the mean and median number of words for all 20 articles.\n",
    "\n",
    "Visualize - Create a Word Cloud using your list of most common words that shows the top 50 (or up to 200) words and a bar chart to show the relative frequencies of the top 15 most frequent words. (Note: these should be words that the average viewer of the website would see, not code from the html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This program uses the first four variables to scrape article text from randomly selected articles linked on the main page\n",
    "# IDS Project 1 Milestone 2\n",
    "# Liam Zalubas\n",
    "# help coming from https://lxml.de/parsing.html#parsers for lxml documentation\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#change this to check more articles\n",
    "articlesToUse:int = 0\n",
    "wordsForCloud:int = 50\n",
    "wordsForGraph:int = 15\n",
    "base_url = \"https://www.cnn.com\"\n",
    "\n",
    "response = requests.get(base_url)\n",
    "# response encoding is utf-8\n",
    "\n",
    "with open(\"main_page.html\", \"wb\") as htmlFile:\n",
    "    htmlFile.write(response.content)\n",
    "\n",
    "# now we have main page downloaded, and we can parse it for article links\n",
    "# THE ENCODING PART HERE IS REALLY IMPORTANT OR ELSE NOTHING WORKS\n",
    "fileWrap = open(\"main_page.html\", \"r\", encoding=\"utf-8\")\n",
    "\n",
    "# this converts the open file into a single string variable\n",
    "fileText: str = fileWrap.read()\n",
    "\n",
    "# turning the downloaded file into a BS4 object\n",
    "bsText = bs(fileText, \"html.parser\")\n",
    "\n",
    "# list to store all valid article links\n",
    "articles = []\n",
    "\n",
    "# get all the potential links that match patterns of articles\n",
    "for link in bsText.find_all('a'):\n",
    "    currentLink: str = link.get('href')\n",
    "    try:\n",
    "        # pattern to save: all articles start with \"/\" and end with \".html\"\n",
    "        if (currentLink.startswith('/') and currentLink.endswith('.html')):\n",
    "            articles.append(base_url+currentLink)\n",
    "    except:\n",
    "        print(\"#\")\n",
    "\n",
    "while (articlesToUse < 1 or articlesToUse > len(articles)):\n",
    "    articlesToUse = int(input(\"How many articles do you want to process?\\n\")) \n",
    "\n",
    "# now that we have all the articles, we pick {randomArticleCount:int} at random and analyze the results\n",
    "# start with a list to put our words and their frequencies in\n",
    "wordsList = {}\n",
    "# and one for the titles to use later\n",
    "titleArray = []\n",
    "#article word counts to use as data for statistics\n",
    "articleWordCounts = [0 for i in range(articlesToUse)]\n",
    "# some resources to use for the method below\n",
    "punc = str.maketrans('', '', string.punctuation)\n",
    "stop_words = stopwords.words(\"english\")\n",
    "# list of words to add to stoplist, including CNN-specific page elements\n",
    "unwantedWords = {\"said\", \"the\", \"feedback\", \"cnn\", \"source\", \"ad\", \"image\", \"video\"}\n",
    "# beautiful soup symbols to remove\n",
    "unwantedThings = {'“', '”', '–', '’s', '—', '’','•'}\n",
    "# specific words to add to stopwords filter\n",
    "for word in unwantedWords:\n",
    "    stop_words.append(word)\n",
    "\n",
    "# method to scrub words and pass them to the adder method\n",
    "def cleanWord(thisWord:str, articleNum:int):\n",
    "    cleanedWord = thisWord\n",
    "    \n",
    "    #remove all numbers: if a number is found, return without adding word\n",
    "    for num in range(0,9):\n",
    "        if (cleanedWord.find(str(num)) != -1):\n",
    "            return\n",
    "\n",
    "    #make it all lowercase and remove most punctuation\n",
    "    cleanedWord = cleanedWord.casefold().translate(punc)\n",
    "    #temporary fix for manually-identified punctuation outliers \n",
    "    for symbol in unwantedThings:\n",
    "        cleanedWord = cleanedWord.replace(symbol, ' ').strip()\n",
    "    #completely ditch the word if it's just an empty string\n",
    "    if (cleanedWord != ''):\n",
    "        #if the above operations turned the word into multiple words, recurse this method for each new word\n",
    "        if (cleanedWord.find(' ') != -1):\n",
    "            cleanedWord.split()\n",
    "            for i in range(len(cleanedWord)):\n",
    "                cleanWord(cleanedWord[i], articleNum)\n",
    "                \n",
    "        # make sure the word is not a stopword before taking it into account\n",
    "        if cleanedWord not in stop_words:\n",
    "            incrementWord(cleanedWord, articleNum)\n",
    "    else:\n",
    "        return\n",
    "\n",
    "# this is a method to send valid words into wordsList or increment their count\n",
    "def incrementWord(thisWord: str, articleNum:int):\n",
    "    # add 1 to the article's word count\n",
    "    articleWordCounts[articleNum] += 1\n",
    "    # for each word key in the dictionary:\n",
    "    # if already exists, add one to the frequency counter\n",
    "    # else, add it to the words list and start at frequency=1\n",
    "    if thisWord in wordsList:\n",
    "        wordsList[thisWord] += 1\n",
    "    else:\n",
    "        wordsList[thisWord] = 1\n",
    "\n",
    "#randomize the links within the list, then use the first {articlesToUse:int} articles.\n",
    "np.random.shuffle(articles)\n",
    "\n",
    "for i in range(articlesToUse):\n",
    "    # turns the entire request page into a BS4 object\n",
    "    thisArticle = bs(requests.get(articles[i]).text, \"html.parser\")\n",
    "    # print the name of the article we're about to check\n",
    "    print(thisArticle.title.text)# type:ignore\n",
    "    titleArray.append((thisArticle.title.text).replace(\" | CNN\",\"\"))# type:ignore\n",
    "    \n",
    "    # now narrow down the text we're interested in to just the article itself\n",
    "    # for each piece of text in a new tag from the last, insert a space between them\n",
    "    textBlocks = thisArticle.article.get_text(strip=True, separator='\\n').splitlines() # type:ignore\n",
    "    \n",
    "    # make it all a single string (separated with spaces)\n",
    "    bigString:str = ''\n",
    "    for block in textBlocks:\n",
    "        bigString += ' ' + block\n",
    "    \n",
    "    # now we tokenize based on single words instead of blocks\n",
    "    bigTokens = nltk.word_tokenize(bigString)\n",
    "    \n",
    "    # then iterate through all tokens we have\n",
    "    for word in bigTokens:\n",
    "        cleanWord(word, i)\n",
    "\n",
    "\n",
    "# Sort the word counts in descending order.\n",
    "sortedWordsList = sorted(wordsList.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "#print the mean and median values for word count\n",
    "print(\"Mean word count:\",np.average(articleWordCounts))\n",
    "for i in range(len(articleWordCounts)):\n",
    "    print(articleWordCounts[i],\"words in article:\",titleArray[i])\n",
    "print(\"Median word count:\",articleWordCounts[int(len(articleWordCounts)/2)])\n",
    "\n",
    "# too many words to fit on the word cloud, here we cut the end of the list\n",
    "while (len(sortedWordsList) > wordsForCloud):\n",
    "    sortedWordsList.pop()\n",
    "\n",
    "# now we make the word cloud with 50 words\n",
    "# Create a WordCloud object.\n",
    "wordcloud = WordCloud()\n",
    "\n",
    "# Generate the word cloud.\n",
    "wordcloud.generate_from_frequencies(wordsList)\n",
    "\n",
    "# Display the word cloud.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Cloud\")\n",
    "plt.show()\n",
    "\n",
    "# now we drop words from the other list for the bar chart\n",
    "while (len(sortedWordsList) > wordsForGraph):\n",
    "    sortedWordsList.pop()\n",
    "\n",
    "for word in sortedWordsList:\n",
    "    print(word)\n",
    "# Create a bar chart.\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.bar([word for word, count in sortedWordsList],\n",
    "        [count for word, count in sortedWordsList])\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Word Frequency Chart\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
