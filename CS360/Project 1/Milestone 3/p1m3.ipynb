{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this milestone is for you to scrape data from an actual website and start to analyze the results. The project should be completed in individually in python without the assistance of any artificial intelligences.\n",
    "\n",
    "Web Scraper - Take all of the text from the top 20 articles coming from one of the following web news sources:\n",
    "\n",
    "espn.com\n",
    "cnn.com\n",
    "goodblacknews.org\n",
    "huffingtonpost.com\n",
    "ign.com\n",
    "theonion.com\n",
    "\n",
    "Statistics - Print a list of the titles of the web pages you are pulling text from and then print the mean and median number of words for all 20 articles.\n",
    "\n",
    "Visualize - Create a Word Cloud using your list of most common words that shows the top 50 (or up to 200) words and a bar chart to show the relative frequencies of the top 15 most frequent words. (Note: these should be words that the average viewer of the website would see, not code from the html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bs4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\liamz\\Documents\\GitHub\\symmetrical-garbanzo\\CS360\\Project 1\\Milestone 3\\p1m3.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/liamz/Documents/GitHub/symmetrical-garbanzo/CS360/Project%201/Milestone%203/p1m3.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# This program uses the first four variables to scrape article text from randomly selected articles linked on the main page\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/liamz/Documents/GitHub/symmetrical-garbanzo/CS360/Project%201/Milestone%203/p1m3.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# IDS Project 1 Milestone 2\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/liamz/Documents/GitHub/symmetrical-garbanzo/CS360/Project%201/Milestone%203/p1m3.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Liam Zalubas\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/liamz/Documents/GitHub/symmetrical-garbanzo/CS360/Project%201/Milestone%203/p1m3.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# help coming from https://lxml.de/parsing.html#parsers for lxml documentation\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/liamz/Documents/GitHub/symmetrical-garbanzo/CS360/Project%201/Milestone%203/p1m3.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbs4\u001b[39;00m \u001b[39mimport\u001b[39;00m BeautifulSoup \u001b[39mas\u001b[39;00m bs\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/liamz/Documents/GitHub/symmetrical-garbanzo/CS360/Project%201/Milestone%203/p1m3.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwordcloud\u001b[39;00m \u001b[39mimport\u001b[39;00m WordCloud\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/liamz/Documents/GitHub/symmetrical-garbanzo/CS360/Project%201/Milestone%203/p1m3.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bs4'"
     ]
    }
   ],
   "source": [
    "# This program uses the first four variables to scrape article text from randomly selected articles linked on the main page\n",
    "# IDS Project 1 Milestone 2\n",
    "# Liam Zalubas\n",
    "# help coming from https://lxml.de/parsing.html#parsers for lxml documentation\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#change this to check more articles\n",
    "articlesToUse:int = 0\n",
    "wordsForCloud:int = 50\n",
    "wordsForGraph:int = 15\n",
    "base_url = \"https://www.cnn.com\"\n",
    "\n",
    "response = requests.get(base_url)\n",
    "# response encoding is utf-8\n",
    "\n",
    "with open(\"main_page.html\", \"wb\") as htmlFile:\n",
    "    htmlFile.write(response.content)\n",
    "\n",
    "# now we have main page downloaded, and we can parse it for article links\n",
    "# THE ENCODING PART HERE IS REALLY IMPORTANT OR ELSE NOTHING WORKS\n",
    "fileWrap = open(\"main_page.html\", \"r\", encoding=\"utf-8\")\n",
    "\n",
    "# this converts the open file into a single string variable\n",
    "fileText: str = fileWrap.read()\n",
    "\n",
    "# turning the downloaded file into a BS4 object\n",
    "bsText = bs(fileText, \"html.parser\")\n",
    "\n",
    "# list to store all valid article links\n",
    "articles = []\n",
    "\n",
    "# get all the potential links that match patterns of articles\n",
    "for link in bsText.find_all('a'):\n",
    "    currentLink: str = link.get('href')\n",
    "    try:\n",
    "        # pattern to save: all articles start with \"/\" and end with \".html\"\n",
    "        if (currentLink.startswith('/') and currentLink.endswith('.html')):\n",
    "            articles.append(base_url+currentLink)\n",
    "    except:\n",
    "        print(\"#\")\n",
    "\n",
    "while (articlesToUse < 1 or articlesToUse > len(articles)):\n",
    "    articlesToUse = int(input(\"How many articles do you want to process?\\n\")) \n",
    "\n",
    "# now that we have all the articles, we pick {randomArticleCount:int} at random and analyze the results\n",
    "# start with a list to put our words and their frequencies in\n",
    "wordsList = {}\n",
    "# and one for the titles to use later\n",
    "titleArray = []\n",
    "#article word counts to use as data for statistics\n",
    "articleWordCounts = [0 for i in range(articlesToUse)]\n",
    "# some resources to use for the method below\n",
    "punc = str.maketrans('', '', string.punctuation)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words.add(\"said\")\n",
    "stop_words.add(\"the\")\n",
    "unwantedThings = {'“', '”', '–', '’s', '—'}\n",
    "\n",
    "# method to scrub words and pass them to the adder method\n",
    "def cleanWord(thisWord:str, articleNum:int):\n",
    "    cleanedWord = thisWord\n",
    "    #remove all numbers: if a number is found, return without adding word\n",
    "    for num in range(0,9):\n",
    "        if (cleanedWord.find(str(num)) != -1):\n",
    "            return\n",
    "    #make it all lowercase and remove most punctuation\n",
    "    cleanedWord = cleanedWord.casefold().translate(punc)\n",
    "    #temporary fix for manually-identified punctuation outliers \n",
    "    for symbol in unwantedThings:\n",
    "        cleanedWord = cleanedWord.replace(symbol, ' ').strip()\n",
    "    #completely ditch the word if it's just an empty string\n",
    "    if (cleanedWord != ''):\n",
    "        #if the above operations turned the word into multiple words, recurse this method for each new word\n",
    "        if (cleanedWord.find(' ') != -1):\n",
    "            cleanedWord.split()\n",
    "            for i in range(len(cleanedWord)):\n",
    "                cleanWord(cleanedWord[i], articleNum)\n",
    "        incrementWord(cleanedWord, articleNum)\n",
    "    else:\n",
    "        return\n",
    "\n",
    "# this is a method to send valid words into wordsList or increment their count\n",
    "def incrementWord(thisWord: str, articleNum:int):\n",
    "    # add 1 to the article's word count\n",
    "    articleWordCounts[articleNum] += 1\n",
    "    # for each word key in the dictionary:\n",
    "    # if already exists, add one to the frequency counter\n",
    "    # else, add it to the words list and start at frequency=1\n",
    "    if thisWord in wordsList:\n",
    "        wordsList[thisWord] += 1\n",
    "    else:\n",
    "        wordsList[thisWord] = 1\n",
    "\n",
    "#randomize the links within the list, then use the first {articlesToUse:int} articles.\n",
    "np.random.shuffle(articles)\n",
    "\n",
    "for i in range(articlesToUse):\n",
    "    # turns the entire request page into a BS4 object\n",
    "    thisArticle = bs(requests.get(articles[i]).text, \"html.parser\")\n",
    "    # print the name of the article we're about to check\n",
    "    print(thisArticle.title.text)# type:ignore\n",
    "    titleArray.append((thisArticle.title.text).replace(\" | CNN\",\"\"))# type:ignore\n",
    "    # now narrow down the text we're interested in to just the article itself\n",
    "    # for each piece of text in a new tag from the last, insert a space between them\n",
    "    textBlocks = thisArticle.article.get_text(strip=True, separator='\\n').splitlines() # type:ignore\n",
    "    # print(textBlocks)\n",
    "    # next I need to tokenize every word I can find, removing punctuation as I go\n",
    "    # for each string in the textBlocks list, convert sentences into single word tokens and add to wordList\n",
    "    # create a translator using string class punctuation table\n",
    "    # for each text block we have, break each down into words and process them\n",
    "    for j in range(len(textBlocks)):\n",
    "        # take apart the whole string into word tokens\n",
    "        wordTokens = textBlocks[j].strip().split()\n",
    "        filtered_tokens = []\n",
    "        for word in wordTokens:\n",
    "            if word not in stop_words:\n",
    "                filtered_tokens.append(word)\n",
    "        # for each token, add it to the list if contains alphabet letters\n",
    "        for k in range(len(filtered_tokens)):\n",
    "            cleanWord(filtered_tokens[k], i)\n",
    "\n",
    "# Sort the word counts in descending order.\n",
    "sortedWordsList = sorted(wordsList.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "#print the mean and median values for word count\n",
    "print(\"Mean word count:\",np.average(articleWordCounts))\n",
    "for i in range(len(articleWordCounts)):\n",
    "    print(articleWordCounts[i],\"words in article:\",titleArray[i])\n",
    "print(\"Median word count:\",articleWordCounts[int(len(articleWordCounts)/2)])\n",
    "\n",
    "# too many words to fit on the word cloud, here we cut the end of the list\n",
    "while (len(sortedWordsList) > wordsForCloud):\n",
    "    sortedWordsList.pop()\n",
    "\n",
    "# now we make the word cloud with 50 words\n",
    "# Create a WordCloud object.\n",
    "wordcloud = WordCloud()\n",
    "\n",
    "# Generate the word cloud.\n",
    "wordcloud.generate_from_frequencies(wordsList)\n",
    "\n",
    "# Display the word cloud.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Cloud\")\n",
    "plt.show()\n",
    "\n",
    "# now we drop words from the other list for the bar chart\n",
    "while (len(sortedWordsList) > wordsForGraph):\n",
    "    sortedWordsList.pop()\n",
    "\n",
    "for word in sortedWordsList:\n",
    "    print(word)\n",
    "# Create a bar chart.\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.bar([word for word, count in sortedWordsList],\n",
    "        [count for word, count in sortedWordsList])\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Word Frequency Chart\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
